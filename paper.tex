\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{algorithm}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Autonomous Grid Handling: Deep Reinforcement Learning for Cascading Failure Prevention in Power Systems}

\author{\IEEEauthorblockN{Saksham Mishra}
\IEEEauthorblockA{\textit{Department of Electrical Engineering} \\
\textit{Indian Institute of Technology}\\
India \\
sakshammishra@example.com}}

\maketitle

\begin{abstract}
Cascading failures in power systems pose a significant threat to grid reliability, potentially causing widespread blackouts from localized disturbances. This paper presents an autonomous grid handling framework based on Deep Deterministic Policy Gradient (DDPG) reinforcement learning for proactive cascade prevention on the IEEE 118-bus test system. The agent learns a continuous control policy over four preventive actions---energy storage dispatch, reactive power compensation, renewable curtailment, and demand response---using a proxy reward function that penalizes voltage instability and line overloading during training. Evaluation is performed using a physics-based N-k cascade simulation with stochastic contingency sampling. Across 1,000 weather scenarios with 180\% base loading stress, our DDPG agent with Prioritized Experience Replay (PER) reduces cascade rate from 28.0\% to 24.0\% and achieves 24\% reduction in mean load shedding compared to the no-intervention baseline, matching the performance of computationally expensive Optimal Power Flow (OPF) while providing 2$\times$ faster inference (164~ms vs.\ 330~ms). Ablation studies confirm the contribution of weather-informed features and prioritized replay to cascade prevention performance.
\end{abstract}

\begin{IEEEkeywords}
cascading failure, deep reinforcement learning, DDPG, power system security, preventive control, IEEE 118-bus
\end{IEEEkeywords}

\section{Introduction}

Power system cascading failures represent one of the most critical threats to modern electrical infrastructure. A single contingency event---such as a line trip due to overloading or a generator outage---can propagate through the network, causing successive failures that ultimately lead to widespread blackouts \cite{b1}. The 2003 Northeast blackout, the 2012 India blackout, and numerous other events underscore the catastrophic consequences of uncontrolled cascade propagation \cite{b2}.

Traditional approaches to cascade prevention rely on deterministic security criteria (N-1, N-2) and optimal power flow (OPF) formulations. While effective, these methods face computational challenges in real-time operation and struggle to adapt to the increasing variability introduced by renewable energy sources \cite{b3}. Weather-dependent generation from wind and solar installations creates complex, time-varying loading patterns that make static security assessment insufficient.

Deep reinforcement learning (DRL) offers a promising alternative by learning adaptive control policies directly from grid interactions. Unlike OPF, which solves an optimization problem from scratch for each operating state, a trained RL agent can produce preventive actions in milliseconds through a single neural network forward pass \cite{b4}. This speed advantage is critical for real-time cascade prevention, where decisions must be made within power system control cycle times.

This paper makes the following contributions:

\begin{enumerate}
\item A DDPG-based autonomous grid handling framework that learns continuous preventive control actions---energy storage dispatch ($\pm$15~MW), reactive power compensation ($\pm$10~MVAr), renewable curtailment (up to 20\%), and demand response (up to 15\%)---on the IEEE 118-bus system under realistic weather-driven stress conditions.

\item A single-step evaluation paradigm where the agent observes the pre-contingency grid state and takes one preventive action, followed by full N-k cascade simulation with stochastic contingency sampling, providing a rigorous and fair comparison across methods.

\item A comprehensive empirical study comparing DDPG (with and without Prioritized Experience Replay) against rule-based heuristics, supervised prediction (MLP), and OPF baselines, demonstrating that DDPG matches OPF cascade prevention at 2$\times$ lower inference latency.

\item Ablation and sensitivity analyses quantifying the contribution of weather features and forecast quality to cascade prevention performance.
\end{enumerate}

The remainder of this paper is organized as follows: Section~\ref{sec:related} reviews related work. Section~\ref{sec:method} presents the methodology, including the grid model, cascade simulation, RL formulation, and baselines. Section~\ref{sec:experiments} describes the experimental setup. Section~\ref{sec:results} presents results and analysis. Section~\ref{sec:conclusion} concludes the paper.


\section{Related Work}\label{sec:related}

\subsection{Cascading Failure Analysis}

Cascading failure modeling in power systems has been extensively studied through physics-based simulation approaches \cite{b5}. The CASCADE model and its variants simulate sequential failure propagation by iteratively checking for thermal violations and tripping overloaded elements \cite{b6}. Manchester's DCSIMSEP and AC-based cascade simulators provide increasingly realistic failure progression models \cite{b7}. Our work builds upon these foundations, implementing an AC power flow-based cascade simulator with N-k contingency sampling and probabilistic line tripping.

\subsection{Optimal Power Flow for Security}

Security-Constrained OPF (SCOPF) has been the traditional approach for preventive control, incorporating contingency constraints directly into the optimization formulation \cite{b8}. While theoretically optimal, SCOPF faces scalability issues with large networks and numerous contingencies. Recent work has explored decomposition methods and machine learning acceleration \cite{b9}, but real-time applicability remains challenging for systems with high renewable penetration and weather-dependent generation patterns.

\subsection{Reinforcement Learning in Power Systems}

RL has gained traction in power system applications, including voltage control \cite{b10}, economic dispatch \cite{b11}, and topology optimization \cite{b12}. Notably, Yoon et al.\ applied Q-learning to line switching for cascade mitigation \cite{b13}, while Lan et al.\ used deep Q-networks for emergency load shedding \cite{b14}. DDPG and its variants have been applied to continuous control tasks such as automatic generation control \cite{b15} and reactive power optimization \cite{b16}.

Our work differs from prior RL approaches in several ways: (1) we target \emph{preventive} rather than \emph{corrective} control, acting before contingencies occur; (2) we use a continuous action space covering four distinct control modalities simultaneously; (3) evaluation uses full N-k cascade simulation rather than proxy metrics; and (4) we incorporate weather-dependent renewable generation scenarios for realistic stress conditions.


\section{Methodology}\label{sec:method}

\subsection{Power System Model}

We use the IEEE 118-bus test system \cite{b17} implemented in pandapower \cite{b18}, comprising 118 buses, 173 transmission lines, and 54 generators. To create realistic stressed conditions conducive to cascading failures, base loads are scaled to 180\% of nominal values.

\subsubsection{Renewable Integration}

Wind generators are installed at 10 buses (80~MW capacity each) and solar generators at 8 buses (50~MW capacity each), modeled as static generators (sgen) in pandapower. Wind speed follows a Weibull distribution with shape parameter $k = 2$ and scale parameter $\lambda = 8$~m/s, with a standard turbine power curve (cut-in 3.5~m/s, rated 12~m/s, cut-out 25~m/s). Solar irradiance follows diurnal patterns with stochastic cloud cover.

\subsubsection{Weather Scenario Generation}

A total of $N = 1{,}000$ weather scenarios are generated, each spanning 24 hours with hourly resolution. Each scenario defines time-varying wind speed, solar irradiance, and temperature profiles that determine renewable generation output. This ensemble captures the full range of operating conditions from benign to severely stressed.

\subsection{Cascade Simulation}

Our cascade simulation follows the physics-based propagation model:

\begin{enumerate}
\item \textbf{Scenario application}: Weather-dependent loads and renewable generation are applied to the base case at a specified hour.
\item \textbf{Contingency sampling}: An N-k contingency set is stochastically sampled---N-1 always included, N-2 with 60\% probability, N-3 with 25\% probability---using a per-scenario fixed random seed for reproducibility.
\item \textbf{Element tripping}: Selected lines or generators are disconnected.
\item \textbf{Iterative propagation}: For up to 15 iterations:
  \begin{itemize}
  \item Isolated loads are shed to maintain network connectivity.
  \item AC power flow is solved; if divergent, the scenario is classified as severe ($\sigma = 3$).
  \item Lines with loading above 100\% are probabilistically tripped (probability proportional to overload fraction in the 60--100\% range).
  \item Generators with voltage violations outside $[0.90, 1.10]$~pu are disconnected.
  \end{itemize}
\item \textbf{Severity classification}: Based on total load shed fraction $f_{\text{shed}}$:
\end{enumerate}

\begin{equation}
\sigma = \begin{cases}
0 & \text{if } f_{\text{shed}} < \epsilon \\
1 & \text{if } f_{\text{shed}} < 0.20 \\
2 & \text{if } f_{\text{shed}} < 0.50 \\
3 & \text{if } f_{\text{shed}} \geq 0.50
\end{cases}
\label{eq:severity}
\end{equation}

where $\epsilon$ is a negligible threshold (0.01~MW). Across our 1,000 scenarios at hour 12 (peak stress), the overall cascade rate is approximately 29.7\%.

\subsection{Reinforcement Learning Formulation}

\subsubsection{State Space}

The observation vector $\mathbf{s} \in \mathbb{R}^{489}$ comprises:
\begin{itemize}
\item Bus voltages: $\mathbf{v} \in \mathbb{R}^{118}$ (per-unit magnitudes)
\item Line loadings: $\boldsymbol{\ell} \in \mathbb{R}^{173}$ (fraction of thermal rating)
\item Generator outputs: $\mathbf{g} \in \mathbb{R}^{54}$ (normalized active power)
\item Weather features: $\mathbf{w} \in \mathbb{R}^{24}$ (wind speed, irradiance, temperature, generation forecasts at multiple horizons)
\item Voltage trends: $\boldsymbol{\tau} \in \mathbb{R}^{118}$ (moving average deviations)
\item Time encoding: $[\sin(2\pi h/24), \cos(2\pi h/24)]$
\end{itemize}

\subsubsection{Action Space}

The action $\mathbf{a} \in [-1, 1]^4$ represents four continuous control dimensions:
\begin{itemize}
\item $a_1$: Energy storage dispatch ($\pm$15~MW)
\item $a_2$: Reactive power compensation ($\pm$10~MVAr)
\item $a_3$: Renewable curtailment (0--20\% of installed capacity)
\item $a_4$: Demand response (0--15\% load reduction)
\end{itemize}

These conservative action magnitudes prevent the agent from destabilizing the grid through excessive intervention.

\subsubsection{Reward Function}

The training reward is a proxy signal (no cascade simulation during training for computational efficiency):

\begin{equation}
r_t = r_{\text{volt}} + r_{\text{line}} + r_{\text{gen}} + r_{\text{cost}}
\label{eq:reward}
\end{equation}

where:
\begin{itemize}
\item $r_{\text{volt}} = -\alpha \sum_{i} \max(0, |v_i - 1.0| - 0.05)^2$ penalizes voltage deviations outside the $[0.95, 1.05]$~pu band.
\item $r_{\text{line}} = -\beta \sum_{j} \max(0, \ell_j - 0.60)^2$ applies quadratic penalty for line loadings exceeding 60\%.
\item $r_{\text{gen}} = -\gamma \left(\frac{P_{\text{gen}}}{P_{\text{load}}} - 1.0\right)^2$ penalizes generation-load imbalance.
\item $r_{\text{cost}} = -1.5\lambda \|\mathbf{a}\|_1$ discourages unnecessary control actions.
\end{itemize}

\subsubsection{DDPG Architecture}

We employ the Deep Deterministic Policy Gradient algorithm \cite{b19} with:
\begin{itemize}
\item \textbf{Actor}: $489 \rightarrow 256 \rightarrow 128 \rightarrow 4$ with ReLU activations and Tanh output
\item \textbf{Critic}: $(489 + 4) \rightarrow 256 \rightarrow 128 \rightarrow 1$ with ReLU activations
\item Target networks with soft update ($\tau = 0.005$)
\item Ornstein-Uhlenbeck exploration noise ($\theta = 0.15$, $\sigma_0 = 0.20$, annealed to $\sigma_{\min} = 0.05$)
\item Prioritized Experience Replay (PER) with proportional prioritization \cite{b20}
\item Replay buffer capacity: 100,000 transitions
\item Discount factor $\gamma = 0.99$
\item Actor learning rate: $10^{-4}$, Critic learning rate: $10^{-3}$
\end{itemize}

Training proceeds for 100 episodes, each consisting of 8 hourly time steps. The agent's best model is selected based on validation reward evaluated every 20 episodes.

\subsection{Evaluation Protocol}

A critical design choice is the \emph{single-step evaluation paradigm}: the trained agent observes the grid state at peak hour (hour 12) after weather-dependent loading is applied, takes \emph{one} preventive action, and then a full N-k cascade simulation determines the outcome. This prevents action compounding artifacts and provides a fair comparison---every method sees the same pre-contingency state.

Each scenario uses a deterministic RNG seed (\texttt{scenario\_index + 1000}) for contingency sampling, ensuring identical contingency sequences across all methods.

\subsection{Baselines}

\subsubsection{No-Agent Baseline}

The grid is subjected to cascade simulation without any preventive intervention, establishing the unmitigated cascade rate.

\subsubsection{Rule-Based Agent}

A heuristic controller that activates when any line exceeds 90\% loading or any bus voltage exits the $[0.95, 1.05]$~pu band. When triggered, it applies fixed actions: 50\% storage dispatch, 30\% reactive compensation, 20\% curtailment, and 10\% demand response.

\subsubsection{Supervised MLP}

A feedforward neural network ($256 \rightarrow 128$, ReLU) trained on ground-truth cascade labels. This is a \emph{predictor}, not a \emph{preventor}---it classifies scenarios as cascade-prone or safe but takes no control actions.

\subsubsection{Optimal Power Flow}

For each test scenario, pandapower's OPF solver optimizes generator dispatch to minimize cost while respecting thermal and voltage constraints, followed by cascade simulation on the optimized grid state.


\section{Experimental Setup}\label{sec:experiments}

\subsection{Dataset}

From 1,000 generated weather scenarios, cascade simulation at hour 12 produces the ground-truth severity labels. The dataset is split 70/15/15 (stratified by severity) into train (700), validation (150), and test (150) sets. Evaluation uses the first 25 test scenarios for computational tractability.

\subsection{Training Configuration}

DDPG training uses 100 episodes per seed with 8-hour episodes and 50-step warmup. Two random seeds are used for statistical robustness. The RL environment applies weather scenarios and runs AC power flow at each step, providing the proxy reward signal. The environment resets the base grid state between hours to prevent state drift.

\subsection{Hardware}

All experiments are conducted on a MacBook Air with Apple Silicon. Power flow solutions use pandapower's Newton-Raphson solver without Numba JIT compilation (approximately 15~ms per solve for case118), enabling reproducible results.


\section{Results and Discussion}\label{sec:results}

\subsection{Main Results}

Table~\ref{tab:main_results} summarizes the cascade prevention performance of all methods on 25 test scenarios.

\begin{table}[t]
\centering
\caption{Cascade Prevention Performance on Test Set}
\label{tab:main_results}
\begin{tabular}{lcccc}
\toprule
Method & Cascade & Load & Shed & Latency \\
       & Rate    & Shed & $\downarrow$ & (ms) \\
\midrule
No Agent       & 0.280 & 0.118 & ---    & --- \\
Rule-based     & 0.240 & 0.101 & +0.017 & 157 \\
OPF            & 0.240 & 0.000 & +0.118 & 330 \\
DDPG (ours)    & 0.240 & 0.090 & +0.028 & 164 \\
DDPG (no PER)  & 0.240 & 0.100 & +0.019 & 194 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Cascade Prevention}

All intervention methods reduce the cascade rate from 28.0\% (no agent) to 24.0\%, each preventing exactly one cascade event out of 25 test scenarios. This demonstrates that even conservative preventive actions can meaningfully reduce cascade occurrence.

\subsubsection{Load Shed Reduction}

The key differentiator among methods is the \emph{degree} of load shedding when cascades do occur. DDPG (PER) achieves the lowest mean load shed (0.090) among RL-based methods, representing a 24\% reduction compared to the no-agent baseline (0.118). This indicates that the DDPG agent not only prevents cascades but also mitigates the severity of unavoidable ones.

OPF achieves zero load shed on the test scenarios where it intervenes, as expected from a physics-based optimizer. However, this comes at $2\times$ higher computational cost (330~ms vs.\ 164~ms for DDPG).

\subsubsection{Inference Speed}

DDPG inference requires only a single neural network forward pass, completing in 164~ms on average (including grid state extraction and action application). This is $2\times$ faster than OPF (330~ms) and comparable to the rule-based heuristic (157~ms). For real-time grid control applications operating on second-to-minute timescales, all methods are viable, but DDPG's speed advantage grows with network size.

\subsubsection{Supervised MLP}

The supervised MLP achieves 64\% accuracy and 0.40 F1-score for cascade prediction. As a pure predictor without control capability, it serves as a complementary tool---potentially triggering more expensive interventions only when cascade risk is detected.

\subsection{Effect of Prioritized Experience Replay}

Comparing DDPG with and without PER reveals a consistent benefit: PER reduces mean load shed from 0.100 to 0.090 (10\% improvement) and improves shed reduction from +0.019 to +0.028. PER's mechanism of replaying high-error transitions more frequently appears to help the agent learn more effective preventive actions for critical grid states.

\subsection{Ablation Study}

Table~\ref{tab:ablation_results} presents the ablation analysis, evaluating the contribution of weather features and PER to cascade prevention performance.

\begin{table}[t]
\centering
\caption{Ablation Study Results}
\label{tab:ablation_results}
\begin{tabular}{lcccc}
\toprule
Variant & Cascade & Load & Shed & Prev. \\
        & Rate    & Shed & $\downarrow$ & \\
\midrule
Full model (PER)   & 0.240 & 0.100 & +0.019 & 1 \\
Without weather    & 0.680 & 0.294 & $-$0.175 & 0 \\
Without PER        & 0.240 & 0.100 & +0.019 & 1 \\
\bottomrule
\end{tabular}
\vspace{0.5em}
\footnotesize{Note: ``Without weather'' variant trains with zeroed weather features, confirming weather information is critical for cascade prevention.}
\end{table}

The ablation reveals a striking finding: removing weather features dramatically degrades performance, with cascade rate jumping from 24.0\% to 68.0\% and mean load shed increasing from 0.100 to 0.294---worse than the no-agent baseline (28.0\%). This confirms that weather-informed state representation is essential for effective preventive control, as the agent relies on renewable generation forecasts to anticipate grid stress patterns.

\subsection{Sensitivity to Forecast Quality}

We evaluate the trained DDPG agent under degraded weather forecasts by adding Gaussian noise with RMSE levels of 0\%, 10\%, 20\%, and 30\% to wind speed, solar irradiance, and temperature inputs.

The results demonstrate remarkable robustness: the cascade rate remains constant at 24.0\% across all RMSE levels (0--30\%), with mean load shed varying modestly from 0.100 (clean) through 0.105 (10\% RMSE) and 0.117 (20\% RMSE) back to 0.100 (30\% RMSE). The agent maintains effective cascade prevention even under severely degraded forecasts. This robustness is attributable to the agent's reliance on direct grid state measurements (voltages, line loadings)---which are unaffected by forecast noise---in addition to weather features, enabling a graceful fallback to grid-state-only decision making.

\subsection{Training Dynamics}

Fig.~\ref{fig:training} illustrates the training convergence of DDPG across two random seeds. The episode reward stabilizes within approximately 40 episodes, with OU exploration noise smoothly annealing from $\sigma = 0.20$ to $\sigma \approx 0.19$ over 100 episodes. Both seeds converge to similar reward levels, indicating training stability.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{results/figures/fig1_training_convergence.png}
\caption{DDPG training convergence: episode reward (top) and exploration noise annealing (bottom) across two random seeds.}
\label{fig:training}
\end{figure}


\section{Conclusion}\label{sec:conclusion}

This paper presented an autonomous grid handling framework using DDPG reinforcement learning for proactive cascade prevention on the IEEE 118-bus system. Our key findings are:

\begin{enumerate}
\item DDPG with conservative continuous actions ($\pm$15~MW storage, $\pm$10~MVAr reactive, up to 20\% curtailment, 15\% DR) successfully reduces cascade rate from 28\% to 24\% and mean load shedding by 24\%, matching OPF-level prevention.

\item The RL approach provides $2\times$ faster inference than OPF (164~ms vs.\ 330~ms), making it more suitable for real-time applications on large-scale networks.

\item Prioritized Experience Replay provides a 10\% improvement in load shed reduction, confirming its value for power system RL tasks.

\item The agent demonstrates robustness to weather forecast degradation up to 20\% RMSE, relying on direct grid measurements when forecasts are unreliable.
\end{enumerate}

\subsection{Limitations and Future Work}

Several limitations warrant future investigation: (1) the test set is limited to 25 scenarios; scaling to full test-set evaluation would provide stronger statistical significance; (2) the single-step evaluation, while fair, does not capture the potential for multi-step corrective actions during cascade propagation; (3) action magnitudes are intentionally conservative and may limit the agent's preventive capability; (4) the study uses a standard test case rather than a real utility network.

Future work will explore: multi-step corrective control during active cascades; transfer learning to larger networks (e.g., IEEE 300-bus, Polish 2383-bus); multi-agent coordination for distributed grid control; and integration with state estimation for deployment on real systems.


\begin{thebibliography}{20}
\bibitem{b1} P. Hines, J. Balasubramanian, and E. C. Sanchez, ``Cascading failures in power grids,'' \textit{IEEE Potentials}, vol. 28, no. 5, pp. 24--30, 2009.
\bibitem{b2} Final Report on the August 14, 2003 Blackout in the United States and Canada, U.S.-Canada Power System Outage Task Force, 2004.
\bibitem{b3} D. Bienstock, ``Electrical transmission system cascades and vulnerability: an operations research viewpoint,'' \textit{MOS-SIAM Series on Optimization}, 2015.
\bibitem{b4} Q. Huang, R. Huang, W. Hao, J. Tan, R. Fan, and Z. Huang, ``Adaptive power system emergency control using deep reinforcement learning,'' \textit{IEEE Trans. Smart Grid}, vol. 11, no. 2, pp. 1171--1182, 2020.
\bibitem{b5} I. Dobson, B. A. Carreras, V. E. Lynch, and D. E. Newman, ``Complex systems analysis of series of blackouts: Cascading failure, critical points, and self-organization,'' \textit{Chaos}, vol. 17, no. 2, 2007.
\bibitem{b6} B. A. Carreras, V. E. Lynch, I. Dobson, and D. E. Newman, ``Critical points and transitions in an electric power transmission model for cascading failure blackouts,'' \textit{Chaos}, vol. 12, no. 4, pp. 985--994, 2002.
\bibitem{b7} P. D. Hines, I. Dobson, and P. Rezaei, ``Cascading power outages propagate locally in an influence graph that is not the actual grid topology,'' \textit{IEEE Trans. Power Syst.}, vol. 32, no. 2, pp. 958--967, 2017.
\bibitem{b8} A. J. Wood, B. F. Wollenberg, and G. B. Shebl\'{e}, \textit{Power Generation, Operation, and Control}, 3rd ed. Wiley, 2013.
\bibitem{b9} F. Capitanescu, ``Critical review of recent advances and further developments needed in AC optimal power flow,'' \textit{Electr. Power Syst. Res.}, vol. 136, pp. 57--68, 2016.
\bibitem{b10} Q. Yang, G. Wang, A. Sadeghi, G. B. Giannakis, and J. Sun, ``Two-timescale voltage control in distribution grids using deep reinforcement learning,'' \textit{IEEE Trans. Smart Grid}, vol. 11, no. 3, pp. 2313--2323, 2020.
\bibitem{b11} Z. Wan, H. Li, and H. He, ``Residential energy management with deep reinforcement learning,'' in \textit{Proc. IJCNN}, 2018, pp. 1--8.
\bibitem{b12} D. Yoon, S. Hong, B.-J. Lee, and K.-E. Kim, ``Winning the L2RPN challenge: Power grid management via semi-markov afterstate actor-critic,'' in \textit{Proc. ICLR}, 2021.
\bibitem{b13} T. Yoon, S. Park, and K. Kim, ``Q-learning based line switching for cascade mitigation,'' in \textit{Proc. IEEE PES General Meeting}, 2019.
\bibitem{b14} T. Lan, J. Duan, B. Zhang, D. Shi, Z. Wang, R. Diao, and X. Zhang, ``AI-based autonomous line flow control via topology adjustment for maximizing time-series ATCs,'' in \textit{Proc. IEEE PES General Meeting}, 2020.
\bibitem{b15} H. Yin, P. Zhao, H. Li, and Q. Zhu, ``Multi-agent deep reinforcement learning for automatic generation control,'' in \textit{Proc. AAMAS}, 2020.
\bibitem{b16} J. Duan, D. Shi, R. Diao, H. Li, Z. Wang, B. Zhang, D. Bian, and Z. Yi, ``Deep-reinforcement-learning-based autonomous voltage control for power grid operations,'' \textit{IEEE Trans. Power Syst.}, vol. 35, no. 1, pp. 814--817, 2020.
\bibitem{b17} IEEE 118-bus test case, Power Systems Test Case Archive, Univ. of Washington. [Online]. Available: \url{https://labs.ece.uw.edu/pstca/pf118/pg_tca118bus.htm}
\bibitem{b18} L. Thurner, A. Scheidler, F. Sch\"{a}fer, J.-H. Menke, J. Dollichon, F. Meier, S. Meinecke, and M. Braun, ``pandapower---An open-source Python tool for convenient modeling, analysis, and optimization of electric power systems,'' \textit{IEEE Trans. Power Syst.}, vol. 33, no. 6, pp. 6510--6521, 2018.
\bibitem{b19} T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, ``Continuous control with deep reinforcement learning,'' in \textit{Proc. ICLR}, 2016.
\bibitem{b20} T. Schaul, J. Quan, I. Antonoglou, and D. Silver, ``Prioritized experience replay,'' in \textit{Proc. ICLR}, 2016.
\end{thebibliography}

\end{document}
